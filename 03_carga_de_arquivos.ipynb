{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.dialects.postgresql import JSONB \n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- LOAD FUNCTION (Simplified: Append-Only) ---\n",
    "def load_json_to_postgres(json_file_path, table_name, engine, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Loads a line-by-line JSON file into a Postgres table.\n",
    "    \n",
    "    This function APPS data. It does NOT truncate the table.\n",
    "    It assumes the table already exists.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path (str): The full path to the .json file.\n",
    "        table_name (str): The destination table name.\n",
    "        engine (sqlalchemy.engine): The SQLAlchemy connection engine.\n",
    "        chunk_size (int): Number of lines to read per chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Starting load for table: '{table_name}'\")\n",
    "\n",
    "    # --- Check if file exists (a simple, essential check) ---\n",
    "    if not os.path.exists(json_file_path):\n",
    "        print(f\"  ERROR: File not found at: {json_file_path}\")\n",
    "        print(f\"  Skipping load for table '{table_name}'.\")\n",
    "        print(\"-\" * 50)\n",
    "        return\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_rows = 0\n",
    "    \n",
    "    # Define special dtypes for 'tb_business'\n",
    "    special_dtypes = {}\n",
    "    if table_name == 'tb_business':\n",
    "        special_dtypes = {'attributes': JSONB, 'hours': JSONB}\n",
    "        \n",
    "    try:\n",
    "        print(f\"  ... Reading file {json_file_path}...\")\n",
    "        \n",
    "        # Loop through the file in chunks\n",
    "        for chunk in pd.read_json(json_file_path, lines=True, chunksize=chunk_size):\n",
    "            \n",
    "            # Load the chunk into the SQL table\n",
    "            chunk.to_sql(\n",
    "                table_name,\n",
    "                con=engine,\n",
    "                if_exists='append', # <-- Key logic: always append\n",
    "                index=False,\n",
    "                dtype=special_dtypes\n",
    "            )\n",
    "            \n",
    "            total_rows += len(chunk)\n",
    "            print(f\"  ... {total_rows} rows loaded into '{table_name}'...\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Load for table '{table_name}' successful.\")\n",
    "        print(f\"Total {total_rows} rows inserted.\")\n",
    "        print(f\"Load time: {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Basic error handling for the load process\n",
    "        print(f\"ERROR during load for table '{table_name}': {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# --- 1. GLOBAL SETTINGS ---\n",
    "DB_PASSWORD = \"DB_PASSWORD\" \n",
    "BASE_DATA_FOLDER = \"D:/Yelp-JSON\" \n",
    "\n",
    "# --- 2. LOAD MAP (Define files and tables) ---\n",
    "# This dictionary maps the source JSON filename to the destination table name.\n",
    "files_to_load = {\n",
    "    \"yelp_academic_dataset_user.json\": \"tb_user\",\n",
    "    \"yelp_academic_dataset_business.json\": \"tb_business\",\n",
    "    \"yelp_academic_dataset_checkin.json\": \"tb_checkin\",\n",
    "    \"yelp_academic_dataset_tip.json\": \"tb_tip\",\n",
    "    \"yelp_academic_dataset_review.json\": \"tb_review\",\n",
    "}\n",
    "\n",
    "# --- 3. DATABASE CONNECTION ---\n",
    "# This script assumes the database (yelp_db) and tables ALREADY EXIST.\n",
    "db_url = f'postgresql://postgres:{DB_PASSWORD}@localhost:5432/yelp_db'\n",
    "engine = None \n",
    "\n",
    "try:\n",
    "    engine = create_engine(db_url)\n",
    "    # Test the connection\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Database connection successful!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not connect to database: {e}\")\n",
    "    print(\"Check DB_PASSWORD or if the Postgres server is running.\")\n",
    "\n",
    "# --- 4. MAIN LOAD LOOP ---\n",
    "if engine:  \n",
    "    print(\"\\nStarting batch load process (APPEND-ONLY)...\")\n",
    "    print(\"NOTE: Run TRUNCATE in DBeaver first if you want a fresh load.\")\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # Loop through the dictionary and load each file\n",
    "    for file_name, table_name in files_to_load.items():\n",
    "        \n",
    "        full_file_path = os.path.join(BASE_DATA_FOLDER, file_name)\n",
    "        \n",
    "        # Call the simplified load function\n",
    "        load_json_to_postgres(full_file_path, table_name, engine)\n",
    "\n",
    "    overall_end_time = time.time()\n",
    "    print(\"\\nBatch load process finished.\")\n",
    "    print(f\"Total operation time: {overall_end_time - overall_start_time:.2f} seconds.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
